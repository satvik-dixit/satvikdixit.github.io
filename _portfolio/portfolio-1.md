---
title: "Speech Emotion Recognition"
excerpt: "Benchmarking the performance of handcrafted features and data-driven features on speech emotion recognition task across multilingual datasets <br>[<a href='https://docs.google.com/presentation/d/1lF714Nn_kKOj0aTPF_HhfZ07S-eFGnogpKLFGIvv8xM/edit?usp=sharing)'>Slides</a>]
[<a href='https://github.com/satvik-dixit/speech_emotion_recognition'>Code</a>]
<br/><img src='/images/SER.png'>"
collection: portfolio
---

Research Internship at Senseable Intelligence Lab, MIT <br>
Advisor: Professor Satrajit Ghosh

The goal in speech emotion recognition is to identify the emotional state of the speaker from an audio recording of their speech. When training the classification models, we can either use 

1. Handcrafted features (based on acoustic descriptors of the signal, such as fundamental frequency or jitter) 
2. Data-driven features (based on deep learning embeddings such as Hubert and Wav2Vec)

 In this project, we benchmarked the performance of handcrafted features and data-driven features across multilingual datasets.

[[Slides](https://docs.google.com/presentation/d/1lF714Nn_kKOj0aTPF_HhfZ07S-eFGnogpKLFGIvv8xM/edit?usp=sharing)]<br>
[[Code](https://github.com/satvik-dixit/speech_emotion_recognition)]
