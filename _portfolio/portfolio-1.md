---
title: "Speech Emotion Recognition"
excerpt: "Benchmarking the performance of handcrafted features (from openSMILE) and data-driven features (from self-supervised learning based models) on speech emotion recognition task across multilingual datasets<br/><img src='/images/SER.png'>"
collection: portfolio
---
The goal is to extract the emotional state of the speaker from an audio recording of their speech. When training the classification models, we can either use handcrafted features (based on acoustic descriptors of the signal, such as fundamental frequency or jitter) or data-driven features (based on deep learning embeddings such as Hubert and Wav2Vec). In this project, we benchmarked the performance of handcrafted features and data-driven features across multilingual datasets.
